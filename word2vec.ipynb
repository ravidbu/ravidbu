{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from nltk.cluster import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('Indeed_business_analytics.csv')\n",
    "data2 = pd.read_csv('Indeed_data_analytics.csv')\n",
    "data3 = pd.read_csv('SimplyHired_business_analytics.csv')\n",
    "data4 = pd.read_csv('SimplyHired_data_analytics.csv')\n",
    "data5 = pd.read_csv('Glassdoor_business_analytics.csv')\n",
    "data6 = pd.read_csv('Glassdoor_data_analytics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title1 = pd.DataFrame(data1['job_title'])\n",
    "JT1 = job_title1.applymap((lambda x: \"\".join(x.split()) if type(x) is str else x)) \n",
    "Job_Title1 = \"\"\n",
    "a = ' '\n",
    "for p_title1 in JT1['job_title']:\n",
    "    if p_title1 != 'NONE':\n",
    "        Job_Title1 += p_title1 + a\n",
    "\n",
    "job_information1 = \"\"\n",
    "for p_information1 in data1['job_information']:\n",
    "    if p_information1 != 'NONE':\n",
    "        job_information1 += p_information1\n",
    "\n",
    "total1 = Job_Title1 + job_information1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title2 = pd.DataFrame(data2['job_title'])\n",
    "JT2 = job_title2.applymap((lambda x: \"\".join(x.split()) if type(x) is str else x)) \n",
    "Job_Title2 = \"\"\n",
    "a = ' '\n",
    "for p_title2 in JT2['job_title']:\n",
    "    if p_title2 != 'NONE':\n",
    "        Job_Title2 += p_title2 + a\n",
    "\n",
    "job_information2 = \"\"\n",
    "for p_information2 in data2['job_information']:\n",
    "    if p_information2 != 'NONE':\n",
    "        job_information2 += p_information2\n",
    "\n",
    "total2 = Job_Title2 + job_information2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title3 = pd.DataFrame(data3['job_title'])\n",
    "JT3 = job_title3.applymap((lambda x: \"\".join(x.split()) if type(x) is str else x)) \n",
    "Job_Title3 = \"\"\n",
    "a = ' '\n",
    "for p_title3 in JT3['job_title']:\n",
    "    if p_title3 != 'NONE':\n",
    "        Job_Title3 += p_title3 + a\n",
    "\n",
    "job_information3 = \"\"\n",
    "for p_information3 in data3['job_information']:\n",
    "    if p_information3 != 'NONE':\n",
    "        job_information3 += p_information3\n",
    "\n",
    "total3 = Job_Title3 + job_information3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title4 = pd.DataFrame(data4['job_title'])\n",
    "JT4 = job_title4.applymap((lambda x: \"\".join(x.split()) if type(x) is str else x)) \n",
    "Job_Title4 = \"\"\n",
    "a = ' '\n",
    "for p_title4 in JT4['job_title']:\n",
    "    if p_title4 != 'NONE':\n",
    "        Job_Title4 += p_title4 + a\n",
    "\n",
    "job_information4 = \"\"\n",
    "for p_information4 in data4['job_information']:\n",
    "    if p_information4 != 'NONE':\n",
    "        job_information4 += p_information4\n",
    "\n",
    "total4 = Job_Title4 + job_information4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title5 = pd.DataFrame(data5['job_title'])\n",
    "JT5 = job_title5.applymap((lambda x: \"\".join(x.split()) if type(x) is str else x)) \n",
    "Job_Title5 = \"\"\n",
    "a = ' '\n",
    "for p_title5 in JT5['job_title']:\n",
    "    if p_title5 != 'NONE':\n",
    "        Job_Title5 += p_title5 + a\n",
    "\n",
    "job_information5 = \"\"\n",
    "for p_information5 in data5['job_information']:\n",
    "    if p_information5 != 'NONE':\n",
    "        job_information5 += p_information5\n",
    "\n",
    "total5 = Job_Title5 + job_information5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title6 = pd.DataFrame(data6['job_title'])\n",
    "JT6 = job_title6.applymap((lambda x: \"\".join(x.split()) if type(x) is str else x)) \n",
    "Job_Title6 = \"\"\n",
    "a = ' '\n",
    "for p_title6 in JT6['job_title']:\n",
    "    if p_title6 != 'NONE':\n",
    "        Job_Title6 += p_title6 + a\n",
    "\n",
    "job_information6 = \"\"\n",
    "for p_information6 in data6['job_information']:\n",
    "    if p_information6 != 'NONE':\n",
    "        job_information6 += p_information6\n",
    "\n",
    "total6 = Job_Title6 + job_information6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = total1 + total2 + total3 + total4 + total5 + total6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaing the text\n",
    "processed_article = total_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "# processed_article = re.sub('lighted', ' ', processed_article )\n",
    "# processed_article = re.sub('groovy', ' ', processed_article )\n",
    "# processed_article = re.sub('pcr', ' ', processed_article )\n",
    "# processed_article = re.sub('wd', ' ', processed_article )\n",
    "# processed_article = re.sub('kampala', ' ', processed_article )\n",
    "# processed_article = re.sub('rooftop', ' ', processed_article )\n",
    "# processed_article = re.sub('emailwork', ' ', processed_article )\n",
    "# processed_article = re.sub('quotes', ' ', processed_article )\n",
    "# processed_article = re.sub('agvq', ' ', processed_article )\n",
    "# processed_article = re.sub('within', ' ', processed_article )\n",
    "# processed_article = re.sub('provide', ' ', processed_article )\n",
    "# processed_article = re.sub('groovy', ' ', processed_article )\n",
    "# processed_article = re.sub('new', ' ', processed_article )\n",
    "# processed_article = re.sub('respa', ' ', processed_article )\n",
    "# processed_article = re.sub('contractcontract', ' ', processed_article )\n",
    "# processed_article = re.sub('noting', ' ', processed_article )\n",
    "# processed_article = re.sub('noticed', ' ', processed_article )\n",
    "# processed_article = re.sub('using', ' ', processed_article )\n",
    "# processed_article = re.sub('meaningful', ' ', processed_article )\n",
    "# processed_article = re.sub('variancesresponsible', ' ', processed_article )\n",
    "# processed_article = re.sub('stevens', ' ', processed_article )\n",
    "# processed_article = re.sub('transit', ' ', processed_article )\n",
    "# processed_article = re.sub('configures', ' ', processed_article )\n",
    "# #processed_article = re.sub('characterize', ' ', processed_article )\n",
    "# processed_article = re.sub('atkinson', ' ', processed_article )\n",
    "# processed_article = re.sub('perhaps', ' ', processed_article )\n",
    "# processed_article = re.sub('awwa', ' ', processed_article )\n",
    "# processed_article = re.sub('follows', ' ', processed_article )\n",
    "# processed_article = re.sub('explained', ' ', processed_article )\n",
    "# processed_article = re.sub('taxes', ' ', processed_article )\n",
    "# processed_article = re.sub('secures', ' ', processed_article )\n",
    "# processed_article = re.sub('antivirus', ' ', processed_article )\n",
    "# processed_article = re.sub('quotit', ' ', processed_article )\n",
    "# processed_article = re.sub('spirited', ' ', processed_article )\n",
    "# processed_article = re.sub('plusprogramming', ' ', processed_article )\n",
    "# processed_article = re.sub('maturity', ' ', processed_article )\n",
    "# processed_article = re.sub('analyticsplatformasaservic', ' ', processed_article )\n",
    "# processed_article = re.sub('signoff', ' ', processed_article )\n",
    "# processed_article = re.sub('working', ' ', processed_article )\n",
    "# processed_article = re.sub('productowne', ' ', processed_article )\n",
    "# #processed_article = re.sub('usinessanalys', ' ', processed_article )\n",
    "# processed_article = re.sub('nw', ' ', processed_article )\n",
    "# processed_article = re.sub('innovistaopportunity', ' ', processed_article )\n",
    "# processed_article = re.sub('unifier', ' ', processed_article )\n",
    "# processed_article = re.sub('pl', ' ', processed_article )\n",
    "# processed_article = re.sub('echnicalb', ' ', processed_article )\n",
    "# processed_article = re.sub('b', ' ', processed_article )\n",
    "# processed_article = re.sub('eniorb', ' ', processed_article )\n",
    "# #processed_article = re.sub('usinessoperationanalys', ' ', processed_article )\n",
    "# #processed_article = re.sub('application', ' ', processed_article )\n",
    "# processed_article = re.sub('ackup', ' ', processed_article )\n",
    "# processed_article = re.sub('iomarins', ' ', processed_article )\n",
    "# processed_article = re.sub('ernetes', ' ', processed_article )\n",
    "# processed_article = re.sub('ilitieso', ' ', processed_article )\n",
    "# processed_article = re.sub('ergen', ' ', processed_article )\n",
    "# processed_article = re.sub('com', ' ', processed_article )\n",
    "# processed_article = re.sub('ility', ' ', processed_article )\n",
    "# processed_article = re.sub('le', ' ', processed_article )\n",
    "# processed_article = re.sub('supporta', ' ', processed_article )\n",
    "# processed_article = re.sub('ementer', ' ', processed_article )\n",
    "# processed_article = re.sub('astic', ' ', processed_article )\n",
    "# processed_article = re.sub('agnostic', ' ', processed_article )\n",
    "# processed_article = re.sub('e', ' ', processed_article )\n",
    "# processed_article = re.sub('ap', ' ', processed_article )\n",
    "# processed_article = re.sub('must', ' ', processed_article )\n",
    "# processed_article = re.sub('us', ' ', processed_article )\n",
    "# processed_article = re.sub('arrays', ' ', processed_article )\n",
    "# processed_article = re.sub('rotations', ' ', processed_article )\n",
    "# processed_article = re.sub('vouchers', ' ', processed_article )\n",
    "# processed_article = re.sub('societal', ' ', processed_article )\n",
    "# processed_article = re.sub('fingerprint', ' ', processed_article )\n",
    "# processed_article = re.sub('jesus', ' ', processed_article )\n",
    "# processed_article = re.sub('societal', ' ', processed_article )\n",
    "# processed_article = re.sub('ransomware', ' ', processed_article )\n",
    "# processed_article = re.sub('writingqualitative', ' ', processed_article )\n",
    "# processed_article = re.sub('expenditures', ' ', processed_article )\n",
    "# processed_article = re.sub('invitations', ' ', processed_article )\n",
    "# processed_article = re.sub('tb', ' ', processed_article )\n",
    "# processed_article = re.sub('salesforc', ' ', processed_article )\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17533\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(all_words, window=50,min_count=2)\n",
    "vocabulary = word2vec.wv.vocab\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('usinessoperationsanalys', 0.9999286532402039),\n",
       " ('data', 0.9999281167984009),\n",
       " ('us', 0.9999280571937561),\n",
       " ('must', 0.9999278783798218),\n",
       " ('information', 0.999927818775177),\n",
       " ('skills', 0.9999273419380188),\n",
       " ('usinessanalyst', 0.9999269247055054),\n",
       " ('service', 0.9999264478683472),\n",
       " ('clients', 0.9999262690544128),\n",
       " ('productowne', 0.9999262690544128)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words1 = word2vec.wv.most_similar('excel')\n",
    "sim_words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('problem', 0.9999046921730042),\n",
       " ('experience', 0.9999032020568848),\n",
       " ('technical', 0.9999032020568848),\n",
       " ('ability', 0.9999029040336609),\n",
       " ('knowledge', 0.9999028444290161),\n",
       " ('management', 0.9999010562896729),\n",
       " ('systems', 0.9999006390571594),\n",
       " ('working', 0.9999002814292908),\n",
       " ('healthcare', 0.999899685382843),\n",
       " ('skills', 0.9998993277549744)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words2 = word2vec.wv.most_similar('sql')\n",
    "sim_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inessanalyst', 0.9999958276748657),\n",
       " ('inessanalys', 0.9999958276748657),\n",
       " ('tb', 0.9999940395355225),\n",
       " ('requirements', 0.9999938607215881),\n",
       " ('rojectmanager', 0.999993622303009),\n",
       " ('position', 0.9999935626983643),\n",
       " ('process', 0.9999935626983643),\n",
       " ('system', 0.9999935030937195),\n",
       " ('application', 0.9999934434890747),\n",
       " ('e', 0.9999933242797852)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words3 = word2vec.wv.most_similar('r')\n",
    "sim_words3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arrays', 0.41970038414001465),\n",
       " ('rotations', 0.3840939700603485),\n",
       " ('renewables', 0.3781626224517822),\n",
       " ('societal', 0.34875595569610596),\n",
       " ('vouchers', 0.3307044804096222),\n",
       " ('jesus', 0.32906436920166016),\n",
       " ('expenditures', 0.32738548517227173),\n",
       " ('fingerprint', 0.32364338636398315),\n",
       " ('wiliness', 0.32102975249290466),\n",
       " ('invitations', 0.3200222849845886)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words4 = word2vec.wv.most_similar('python')\n",
    "sim_words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = vocabulary\n",
    "\n",
    "# def sent_vectorizer(sent, word2vec):\n",
    "#     sent_vec =[]\n",
    "#     numw = 0\n",
    "#     for w in sent:\n",
    "#         try:\n",
    "#             if numw == 0:\n",
    "#                 sent_vec = word2vec[w]\n",
    "#             else:\n",
    "#                 sent_vec = np.add(sent_vec, word2vec[w])\n",
    "#             numw+=1\n",
    "#         except:\n",
    "#             pass\n",
    "     \n",
    "#     return np.asarray(sent_vec) / numw\n",
    "  \n",
    "# X=[]\n",
    "# for sentence in total_text:\n",
    "#     X.append(sent_vectorizer(sentence, word2vec))   \n",
    "\n",
    "\n",
    "# NUM_CLUSTERS = 4\n",
    "# kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "# assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    " \n",
    "# model = TSNE(n_components=2, random_state=0)\n",
    "# np.set_printoptions(suppress=True)\n",
    " \n",
    "# Y = model.fit_transform(X)\n",
    " \n",
    "# plt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)\n",
    " \n",
    "# for j in range(len(total_text)):    \n",
    "#     plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n",
    "#     print (\"%s %s\" % (assigned_clusters[j],  sentences[j]))\n",
    " \n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
